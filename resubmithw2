#import libraries
import os
import numpy as np
import time
import tensorflow as tf
from matplotlib import pyplot as plt
import keras
import matplotlib.pyplot as plt
%matplotlib inline
np.random.seed(136)
tf.random.set_seed(136)
#setting parameters
size_input = 784
size_hidden_1 = 256
size_hidden_2 = 128
size_output = 10
#number_of_train_examples = 50000
#number_of_test_examples = 10000
batch_size=50
#read in the data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train=tf.reshape(x_train,[x_train.shape[0],-1])
x_test=tf.reshape(x_test,[x_test.shape[0],-1])
# Split dataset into batches
seed=tf.random.set_seed(136)
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=seed).batch(batch_size)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)
# Define class to build mlp model
class MLP(object):
  def __init__(self, size_input, size_hidden_1,size_hidden_2, size_output, device=None):
    """
    size_input: int, size of input layer
    size_hidden: int, size of hidden layer
    size_output: int, size of output layer
    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution
    """
    self.size_input, self.size_hidden_1,self.size_hidden_2, self.size_output, self.device =\
    size_input, size_hidden_1,size_hidden_2, size_output, device
    
    # Initialize weights between input layer and hidden layer 1
    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden_1]))
    # Initialize biases for hidden layer 1
    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden_1]))
     # Initialize weights between hidden layer 1 and  hidden layer 2
    self.W2 = tf.Variable(tf.random.normal([self.size_hidden_1, self.size_hidden_2]))
    # Initialize biases for hidden layer 2
    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden_2]))
    # Initialize weights between hidden layer 2 and output
    self.W3 = tf.Variable(tf.random.normal([self.size_hidden_2, size_output]))
    # Initialize biases for output layer
    self.b3 = tf.Variable(tf.random.normal([1, size_output]))
    
    
    # Define variables to be updated during backpropagation
    self.variables = [self.W1, self.W2, self.W3,self.b1, self.b2,self.b3]
    
  def forward(self, X):
    """
    forward pass
    X: Tensor, inputs
    """
    if self.device is not None:
      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):
        self.y = self.compute_output(X)
    else:
      self.y = self.compute_output(X)
      
    return self.y
  
  def loss(self, y_pred, y_true):
    '''
    y_pred - Tensor of shape (batch_size, size_output)
    y_true - Tensor of shape (batch_size, size_output)
    '''
    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)
    #y_pred_tf = tf.cast(y_pred, dtype=tf.float32)
    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, y_pred)
    #l1=tf.keras.regularizers.L1(l1=0.00001)
    #l2l2=tf.keras.regularizers.L1L2(l1=0.001, l2=0.001)
    #regul_l1=l1(self.variables)
    #regul_l1l2=l2l2(self.variables)
    #loss_l1=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)+regul_l1
    #loss_l112=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)+ regul_l1l2
    return loss
  
  def backward(self, X_train, y_train):
    """
    backward pass
    """
    lr = tf.compat.v1.train.exponential_decay(
        0.001,
        0,
        number_of_train_examples/20, 0.99,
        staircase=True)
    optimizer = tf.keras.optimizers.SGD(lr)
    with tf.GradientTape() as tape:
      predicted = self.forward(X_train)
      current_loss = self.loss(predicted, y_train)
    grads = tape.gradient(current_loss, self.variables)
    optimizer.apply_gradients(zip(grads, self.variables))
        
        
  def compute_output(self, X):
    """
    Custom method to obtain output tensor during forward pass
    """
    # Cast X to float32
    X_tf = tf.cast(X, dtype=tf.float32)      
    # Compute values in hidden layer 
    what1 = tf.matmul(X_tf, self.W1) + self.b1
    hhat1 = tf.nn.relu(what1)
    what2 = tf.matmul(hhat1, self.W2) + self.b2
    hhat2 = tf.nn.relu(what2)
    # Compute output
    output = tf.matmul(hhat2, self.W3) + self.b3
    return output
    #setting epoch
    NUM_EPOCHS = 30
    # Initialize model using CPU
mlp_on_cpu = MLP(size_input, size_hidden_1,size_hidden_2, size_output, device='cpu')

time_start = time.time()
graph_df = pd.DataFrame(columns = ['Epoch', 'Train_Loss', 'Train_Accuracy', 'Test_Loss', 'Test_Accuracy'])
for epoch in range(NUM_EPOCHS):
  loss_total = tf.zeros([1,1], dtype=tf.float32)
  lt = 0
  Accuracy = []
  for inputs, outputs in train_ds:
    preds = mlp_on_cpu.forward(inputs)
    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)
    lt = lt + mlp_on_cpu.loss(preds, outputs)
    mlp_on_cpu.backward(inputs, outputs)
    Train_a=tf.keras.metrics.sparse_categorical_accuracy(outputs, preds)
    Accuracy.append(Train_a)
    mean_accuracy = np.mean(Accuracy)
    mean_loss = np.sum(loss_total) / X_train.shape[0]
    graph_df.loc[epoch, ['Epoch', 'Train_Loss','Train_Accuracy']] = [epoch + 1,mean_loss, mean_accuracy]
  print('Number of Epoch = {} - Average train cross entropy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))
  test_loss_total = tf.Variable(0, dtype=tf.float32)
  test_Accuracy = []
  for inputs, outputs in test_ds:
        preds = mlp_on_cpu.forward(inputs)
        test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)
        Test_a=tf.keras.metrics.sparse_categorical_accuracy(outputs, preds)
        test_Accuracy.append(Test_a)
        mean_test_accuracy = np.mean(test_Accuracy)
        test_mean_loss = np.sum(test_loss_total) / X_test.shape[0]
        graph_df.loc[epoch, ['Epoch','Test_Loss' 'Test_Accuracy']] = [epoch + 1,test_mean_loss, mean_test_accuracy]
  print('Number of Epoch = {} - Average test cross entropy:= {}'.format(epoch + 1, np.sum(test_loss_total) / X_train.shape[0]))

time_taken = time.time() - time_start

print('\nTotal time taken (in seconds): {:.2f}'.format(time_taken))
#For per epoch_time = Total_Time / Number_of_epochs
graph_df
# train and test datasets's results
plt.figure(figsize=(25, 15))
plt.xlabel('EPOCH')
plt.ylabel('ACCURACY') 
plt.title("Accuracy of Train and Test") 
plt.plot(graph_df['Epoch'], graph_df['Train_Accuracy'],label = 'TRAIN')
plt.plot(graph_df['Epoch'], graph_df['Test_Accuracy'l,label = 'TEST'])
plt.plot(graph_df['Epoch'], graph_df['Train_Loss'],label = 'TRAIN')
plt.plot(graph_df['Epoch'], graph_df['Test_Loss'l,label = 'TEST'])

    
